import os
import pandas as pd
from dotenv import load_dotenv
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import re

from config import ExperimentConfig
from prompts.templates import TEMPLATES
from utils.experiment import ExperimentRunner

def main():
    # API í‚¤ ë¡œë“œ
    load_dotenv()
    api_key = os.getenv('UPSTAGE_API_KEY')
    if not api_key:
        raise ValueError("API key not found in environment variables")
    
    # ê¸°ë³¸ ì„¤ì • ìƒì„±
    base_config = ExperimentConfig(template_name='basic')
    
    # ë°ì´í„° ë¡œë“œ
    train = pd.read_csv(os.path.join(base_config.data_dir, 'train.csv'))
    test = pd.read_csv(os.path.join(base_config.data_dir, 'test.csv'))
    
    # í† ì´ ë°ì´í„°ì…‹ ìƒì„±
    toy_data = train.sample(n=base_config.toy_size, random_state=base_config.random_seed).reset_index(drop=True)
    
    # train/valid ë¶„í• 
    train_data, valid_data = train_test_split(
        toy_data,
        test_size=base_config.test_size,
        random_state=base_config.random_seed
    )
    
    # ëª¨ë“  í…œí”Œë¦¿ìœ¼ë¡œ ì‹¤í—˜
    results = {}
    for template_name in tqdm(TEMPLATES.keys(), desc="í…œí”Œë¦¿ ì‹¤í—˜ ì§„í–‰", ncols=80):
        config = ExperimentConfig(
            template_name=template_name,
            temperature=0.0,
            batch_size=4,
            experiment_name=f"toy_experiment_{template_name}"
        )
        runner = ExperimentRunner(config, api_key)
        result = runner.run_template_experiment(train_data, valid_data)

        # ê²°ê³¼ ì €ì¥
        results[template_name] = {
            "train_recall": result["train_recall"],
            "valid_recall": result["valid_recall"]
        }

        # ğŸ”½ train êµì • ê²°ê³¼ ì €ì¥
        train_inputs = train_data[["id", "err_sentence", "cor_sentence"]].reset_index(drop=True)
        train_inputs = train_inputs.rename(columns={"cor_sentence": "answer"})
        train_inputs["cor_sentence"] = result["train_results"]["cor_sentence"]
        train_inputs.to_csv(f"outputs/train_results{template_name}.csv", index=False)  
    # ê²°ê³¼ ë¹„êµ
    print("\n=== í…œí”Œë¦¿ë³„ ì„±ëŠ¥ ë¹„êµ ===")
    for template_name, result in results.items():
        print(f"\n[{template_name} í…œí”Œë¦¿]")
        print("Train Recall:", f"{result['train_recall']['recall']:.2f}%")
        print("Train Precision:", f"{result['train_recall']['precision']:.2f}%")
        print("\nValid Recall:", f"{result['valid_recall']['recall']:.2f}%")
        print("Valid Precision:", f"{result['valid_recall']['precision']:.2f}%")
    
    # ìµœê³  ì„±ëŠ¥ í…œí”Œë¦¿ ì°¾ê¸°
    best_template = max(
        results.items(), 
        key=lambda x: x[1]['valid_recall']['recall']
    )[0]
    
    print(f"\nìµœê³  ì„±ëŠ¥ í…œí”Œë¦¿: {best_template}")
    print(f"Valid Recall: {results[best_template]['valid_recall']['recall']:.2f}%")
    print(f"Valid Precision: {results[best_template]['valid_recall']['precision']:.2f}%")

    # ìµœê³  ì„±ëŠ¥ í…œí”Œë¦¿ìœ¼ë¡œ ì˜ˆì‹œ ë¬¸ì¥ ëª‡ ê°œ ìƒì„±í•´ì„œ ì¶œë ¥
    print("\n[ìµœê³  í…œí”Œë¦¿ ì˜ˆì‹œ ì‘ë‹µ]")
    sample_inputs = test["err_sentence"].head(3).tolist()

    preview_config = ExperimentConfig(
        template_name=best_template,
        temperature=0.0,
        batch_size=4,
        experiment_name="preview_generation"
    )
    preview_runner = ExperimentRunner(preview_config, api_key)

    # ì„ì‹œ DataFrame êµ¬ì„±
    preview_df = test[["id", "err_sentence"]].head(3).reset_index(drop=True)
    preview_outputs = preview_runner.run(preview_df)

    # ê²°ê³¼ ì¶œë ¥
    for err, cor in zip(sample_inputs, preview_outputs["cor_sentence"]):
        print(f"\n[ì…ë ¥] {err}\n[ì¶œë ¥] {cor}")

    # ìµœê³  ì„±ëŠ¥ í…œí”Œë¦¿ìœ¼ë¡œ ì œì¶œ íŒŒì¼ ìƒì„±
    print("\n=== í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘ ===")
    config = ExperimentConfig(
        template_name=best_template,
        temperature=0.0,
        batch_size=4,
        experiment_name="final_submission2"
    )
    
    runner = ExperimentRunner(config, api_key)
    test_results = runner.run(test)
    # ë¬¸ì¥ë¶€í˜¸ ë’¤ ê³µë°± ì œê±° í•¨ìˆ˜
    def remove_trailing_space_after_punctuation(text):
        # 1. ë¬¸ìì—´ ëì— ë¶™ì€ ëª¨ë“  ìœ ë‹ˆì½”ë“œ ê³µë°± ë¬¸ì ì œê±°
        return re.sub(r'[\s\u200b\u200c\u200d\ufeff]+$', '', text)

    # ë¬¸ì¥ë¶€í˜¸ ë’¤ ê³µë°± ì œê±° ì ìš©
    test_results["cor_sentence"] = test_results["cor_sentence"].apply(remove_trailing_space_after_punctuation)
    # sample_submission í˜•ì‹ì— ë§ê²Œ ìƒì„±
    output = test.copy()
    output["cor_sentence"] = test_results["cor_sentence"]
    output = output[["id", "err_sentence", "cor_sentence"]]
    output.to_csv("outputs/submission_multiturn2.csv", index=False)

    print("\nì œì¶œ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: submission_multiturn2.csv")
    print(f"ì‚¬ìš©ëœ í…œí”Œë¦¿: {best_template}")
    print(f"ì˜ˆì¸¡ëœ ìƒ˜í”Œ ìˆ˜: {len(output)}")

if __name__ == "__main__":
    main()